------------------------------
# 1 - Explication:
------------------------------

### **D√©monstration avec un seul alpha (Œ± = 0.5)**

---

Ce code est une d√©monstration d'un agent Q-Learning qui apprend √† r√©soudre l'environnement **MountainCar-v0** en utilisant **une seule valeur de param√®tre alpha (Œ±)**, ici **Œ± = 0.5**. L'agent est form√© pendant 2000 √©pisodes, et ensuite √©valu√© sur 10 essais.

#### üí° **Alpha (Œ± = 0.5)**

---

### **Explication du code :**

1. **Environnement de simulation :**  
   L'environnement **MountainCar-v0** de **OpenAI Gym** est utilis√©. Dans cet environnement, une voiture doit apprendre √† gravir une colline en prenant de l'√©lan.

2. **Agent Q-Learning (avec Œ± = 0.5) :**  
   - L'agent Q-Learning est cr√©√© avec un taux d'apprentissage (alpha) de **0.5**.  
   - L'agent apprend √† s√©lectionner des actions optimales pour maximiser ses r√©compenses. 
   - La table Q est mise √† jour en fonction des r√©compenses re√ßues apr√®s chaque action.

3. **Phases du programme :**

   - **Phase d'apprentissage :**  
     L'agent s'entra√Æne pendant **2000 √©pisodes**. Pendant cette phase, il explore diff√©rentes actions pour apprendre √† ma√Ætriser l'environnement.
   
   - **Phase d'√©valuation :**  
     Apr√®s l'apprentissage, l'agent est √©valu√© sur **10 essais**. Pendant chaque essai, vous verrez la voiture se d√©placer en temps r√©el dans une fen√™tre **gym**. L'objectif de l'agent est de monter la colline en **moins de 200 pas de temps**.
   
   - **Suivi des r√©sultats :**  
     Pour chaque essai, le nombre de **pas de temps** est enregistr√©, ainsi que le **succ√®s** ou l'**√©chec** de l'agent (r√©ussir √† monter la colline en moins de 200 pas). Les r√©sultats sont affich√©s √† la fin sous forme de graphiques.

4. **Affichage des r√©sultats :**
   - **Graphique des succ√®s/√©checs** : Un graphique montre si l'agent a r√©ussi ou √©chou√© pour chaque essai.
   - **Graphique du nombre de pas** : Un autre graphique montre le nombre de pas n√©cessaires pour chaque essai.
   - **Statistiques globales** : Le taux de succ√®s, la moyenne et la m√©diane des pas sont calcul√©s et affich√©s en bas des graphiques.

### **D√©tails des param√®tres :**
- **Episodes d'entra√Ænement :** 2000
- **Nombre d'actions possibles :** 3 (aller √† gauche, ne rien faire, aller √† droite)
- **Nombre d'essais d'√©valuation :** 10
- **Alpha (lr - taux d'apprentissage) :** 0.5

### **Ce que vous verrez dans ce code :**
- Pendant la phase d'√©valuation, la voiture se d√©placera en **temps r√©el** dans l'environnement **MountainCar-v0** pendant que l'agent essaie d'appliquer ce qu'il a appris. Apr√®s chaque essai, les performances de l'agent sont collect√©es, et vous verrez :
  - **Un graphique des succ√®s et √©checs** (l'agent a-t-il r√©ussi ou √©chou√© √† chaque essai ?).
  - **Un graphique du nombre de pas n√©cessaires** pour chaque essai.

### **Ex√©cution finale :**
- √Ä la fin des essais, vous obtiendrez un r√©sum√© des performances de l'agent avec un taux de succ√®s et des statistiques de pas de temps.

---

üí° **Conclusion** : Ce code est une d√©monstration simple avec un seul **alpha**. Il montre comment un agent peut apprendre et appliquer les concepts de **Q-Learning** pour r√©soudre un probl√®me de type **MountainCar** en ajustant un seul param√®tre cl√©, le **taux d'apprentissage (alpha)**.



------------------------------
# 2 - D√©monstration
------------------------------

```
git clone https://github.com/hrhouma/RLCode2-1.git
cd RLCode2-1
python3 -m venv mountain_car_env (ici python3 et non python)
mountain_car_env\Scripts\activate
python nom_du_script.py  (Ici python et non python3)
pip install -r requirements.txt
python main.py
deactivate
```


# 1. Cloner le projet

Commencez par cloner le projet depuis le d√©p√¥t GitHub :

```bash
git clone https://github.com/hrhouma/RLCode2.git
```

Acc√©dez au dossier du projet :

```bash
cd RLCode2
code .
```

# 2. Cr√©ation de l'environnement virtuel

Cr√©ez un environnement virtuel nomm√© `mountain_car_env` :

```bash
python3 -m venv mountain_car_env
```

# 3. Activation de l'environnement virtuel

Activez l'environnement :

- Sur macOS et Linux :
  ```bash
  source mountain_car_env/bin/activate
  ```

- Sur Windows :
  ```bash
  mountain_car_env\Scripts\activate
  ```

# 4. Installation des d√©pendances

Installez les packages n√©cessaires :

```bash
pip install -r requirements.txt
```

# 5. Structure du projet

Le projet est d√©j√† organis√© dans le d√©p√¥t que vous avez clon√©. Il comprend les fichiers suivants :

- `helpers.py`
- `QLearning.py`
- `main.py`

# 6. Ex√©cution du projet

Avec l'environnement virtuel activ√©, ex√©cutez le fichier `main.py` pour lancer le projet :

```bash
python main.py
```

# 7. D√©sactivation de l'environnement virtuel

Une fois que vous avez termin√©, vous pouvez d√©sactiver l'environnement avec la commande suivante :

```bash
deactivate
```

---------------------------
# 3 - Annexe :
---------------------------



## 1. Cr√©ation de l'environnement virtuel

Ouvrez un terminal et naviguez vers le dossier de votre projet. Cr√©ez un environnement virtuel nomm√© "mountain_car_env" :

```bash
python3 -m venv mountain_car_env
```

## 2. Activation de l'environnement virtuel

Activez l'environnement :

- Sur macOS et Linux :
```bash
source mountain_car_env/bin/activate
```

- Sur Windows :
```bash
mountain_car_env\Scripts\activate
```

## 3. Installation des d√©pendances

Installez les packages n√©cessaires :

```bash
pip install numpy pandas gym
```

## 4. Cr√©ation du fichier requirements.txt

Cr√©ez un fichier `requirements.txt` √† la racine de votre projet avec le contenu suivant :

```
numpy==1.23.5
pygame
pandas
gym
matplotlib
```

Alternativement, vous pouvez g√©n√©rer ce fichier automatiquement :

```bash
pip freeze > requirements.txt
pip install -r requirements.txt
```

## 5. Structure du projet

Organisez votre projet avec trois fichiers Python :

- `helpers.py`
- `QLearning.py`
- `main.py`


## 6. Ex√©cution du projet

Dans le terminal, avec l'environnement virtuel activ√©, ex√©cutez :

```bash
python main.py
```

## 7. D√©sactivation de l'environnement virtuel

Une fois termin√©, d√©sactivez l'environnement :

```bash
deactivate
```

## 8. Partage du projet

Pour partager ou reproduire l'environnement sur un autre syst√®me :

1. Incluez le fichier `requirements.txt` dans votre projet.
2. L'autre personne peut cr√©er un nouvel environnement virtuel et installer les d√©pendances avec :

```bash
python3 -m venv mountain_car_env
source mountain_car_env/bin/activate  # ou mountain_car_env\Scripts\activate sur Windows
pip install -r requirements.txt
```

En suivant ces √©tapes, vous aurez un projet bien structur√©, utilisant un environnement virtuel, avec toutes les corrections n√©cessaires pour faire fonctionner le code de l'agent Q-Learning pour le probl√®me Mountain Car.

----------------------------
# Annexe 
----------------------------

- Le code que vous avez ci-haut met en ≈ìuvre un **agent d'apprentissage par renforcement bas√© sur le Q-Learning** pour r√©soudre le probl√®me **MountainCar-v0** propos√© par OpenAI Gym. 
- Je vous propose une description √©tape par √©tape de ce que fait le code et ce que vous verrez en l'ex√©cutant :

### 1. **Probl√®me √† r√©soudre : MountainCar-v0**
Le probl√®me **MountainCar** est un environnement classique o√π une voiture doit atteindre le sommet d'une colline. Cependant, la voiture n'a pas assez de puissance pour monter directement la pente, donc elle doit d'abord reculer pour prendre de l'√©lan avant d'avancer.

- **Objectif** : Amener la voiture au sommet de la colline.
- **Actions possibles** : La voiture peut aller √† gauche, ne rien faire (aucune pouss√©e), ou aller √† droite.
- **√âtats** : Chaque √©tat correspond √† la position et √† la vitesse de la voiture.

### 2. **Composants cl√©s du code :**

#### a) **QLearning.py :** Impl√©mente l'agent Q-Learning
- **Classe QLAgent** : Repr√©sente l'agent de Q-Learning, qui apprend les actions optimales √† prendre dans chaque √©tat. Il utilise une **table de Q-valeurs** pour stocker les valeurs de r√©compense pour chaque paire (√©tat, action).
  - **M√©thodes principales :**
    - `act(state, episode)` : Choisit l'action optimale √† partir de l'√©tat actuel.
    - `updateQ(s, a, r, s_dash)` : Met √† jour la table de Q-valeurs apr√®s chaque action.
    - `learn(episodes)` : Entra√Æne l'agent sur un certain nombre d'√©pisodes.
  
#### b) **helpers.py :** G√®re la discr√©tisation des √©tats continus
- Les √©tats du probl√®me MountainCar (position et vitesse) sont continus. Ce fichier contient des fonctions pour transformer ces √©tats continus en √©tats discrets, ce qui permet √† l'algorithme de Q-Learning de les manipuler.
  - `discretize(state)` : Prend un √©tat continu (position, vitesse) et le transforme en un num√©ro d'√©tat discret.

#### c) **main.py :** Entra√Æne et √©value l'agent Q-Learning
- **Entra√Ænement** : L'agent est entra√Æn√© pendant 2000 √©pisodes sur l'environnement **MountainCar-v0**. Pendant chaque √©pisode, il choisit des actions, met √† jour ses Q-valeurs et tente de r√©soudre le probl√®me en atteignant le sommet de la colline.
- **√âvaluation** : Apr√®s l'entra√Ænement, l'agent est √©valu√© sur 10 essais pour voir combien de fois il r√©ussit √† atteindre le sommet dans moins de 200 pas de temps.

#### d) **requirements.txt :** Contient les d√©pendances n√©cessaires pour ex√©cuter le projet
- Ce fichier liste les biblioth√®ques Python requises : `numpy`, `gym`, `pandas`, `matplotlib`, etc.

### 3. **Ce que vous allez voir en l'ex√©cutant :**

- **Visualisation graphique** : Lorsque vous ex√©cuterez le projet, vous verrez une fen√™tre graphique montrant l'environnement **MountainCar-v0**, o√π une voiture essaie de monter une colline en oscillant d'avant en arri√®re.
  - Pendant l'entra√Ænement, l'environnement n'est pas affich√©, mais lors des essais d'√©valuation, vous verrez la voiture essayer de grimper la colline en se balan√ßant.

- **Statistiques finales** :
  - Vous verrez un graphique avec deux sous-graphiques :
    1. **Succ√®s/√âchecs des essais** : Chaque point indique si l'agent a r√©ussi (1) ou √©chou√© (0) √† atteindre le sommet lors de chaque essai d'√©valuation.
    2. **Nombre de pas de temps par essai** : Le nombre de pas de temps n√©cessaires pour terminer chaque essai (plus le nombre est petit, plus l'agent est performant).
  - Un texte montrera √©galement les statistiques globales, comme le taux de succ√®s et la moyenne des pas de temps.

### 4. **R√©sultats attendus :**
- Si l'agent apprend correctement, vous verrez un **taux de succ√®s** √©lev√© lors des essais d'√©valuation apr√®s l'entra√Ænement.
- Vous devriez √©galement voir une **r√©duction progressive** du nombre de pas de temps n√©cessaires pour atteindre le sommet, indiquant que l'agent apprend √† r√©soudre le probl√®me plus efficacement.

### 5. **D√©tails suppl√©mentaires :**
- **Exploration vs Exploitation** : Au d√©but de l'entra√Ænement, l'agent ajoutera du bruit √† ses d√©cisions pour explorer diff√©rentes actions. √Ä mesure que l'entra√Ænement progresse, il deviendra plus s√ªr de ses d√©cisions et explorera moins.
  
- **Utilisation d'un environnement virtuel** : Le fichier `requirements.txt` vous permet d'installer facilement les d√©pendances n√©cessaires dans un environnement virtuel, pour assurer une ex√©cution sans conflit de biblioth√®ques.

En r√©sum√©, ce code met en ≈ìuvre un agent Q-Learning pour r√©soudre le probl√®me MountainCar, avec des visualisations √† la fin pour √©valuer ses performances.
